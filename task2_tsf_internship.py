# -*- coding: utf-8 -*-
"""Task2_TSF_internship.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gQ3wAhR24pqEAQjmdst6b-M5v7ydvaBT

D. Jayanth Srinivas

# Prediction using Unsupervised ML

predicting the optimum number of clusters
"""

# import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets

#@title loading the data

iris = datasets.load_iris()
iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)

#@title checking the dataframe

iris_df.head()

#@title understanding data

iris_df.describe()

#@title visualizing the data

import seaborn as sns
bp = sns.boxplot(data=iris_df)
bp.set_xticklabels(iris_df.columns,size=8)

plt.show()

"""**we see few outliers in the sepal width column. lets go ahead and remove the data points corresponding to these outliers**"""

#@title removing outliers

q1 = iris_df['sepal width (cm)'].quantile(0.25)       # first quartile
q3 = iris_df['sepal width (cm)'].quantile(0.75)       # third quartile

IQR = q3-q1                   # inter quartile range 

iris_df = iris_df[(iris_df['sepal width (cm)'] > (q1 - 1.5*IQR)) & (iris_df['sepal width (cm)'] < (q3 + 1.5*IQR)) ] # removing data points which are not in range [q1 - 1.5*IQR , q3 + 1.5*IQR]

#@title visualizing after removing outliers

bp = sns.boxplot(data=iris_df)
bp.set_xticklabels(iris_df.columns,size=8)

plt.show()

#@title lets visualize the correlation between the variables of our dataset using pairplot function.


sns.pairplot(iris_df)

"""**looks like petal length and petal width are highly correlated positively**"""

#@title finding the optimum number clusters using the elbow method

x = iris_df.values

from sklearn.cluster import KMeans

sswc = []               # sum of squares within cluster

for i in range(1,10):
  kmeans = KMeans(n_clusters = i , init= 'k-means++', max_iter = 300, n_init = 10 , random_state = 0)
  kmeans.fit(x)
  sswc.append(kmeans.inertia_)

# visualizing the elbow method

plt.plot(range(1,10),sswc)
plt.xlabel('number of clusters')
plt.ylabel('sum of squares within cluster')
plt.title('ELBOW METHOD')
plt.show()

"""**As one can see, the elbow occurs when the number of clusters is equal to 3. That is when the sum of squares within clusters does not decrease much with increase in number of clusters**"""

#@title Setting number of clusters to be 3 and fitting the data into the model

kmeans = KMeans(n_clusters = 3 , init= 'k-means++', max_iter = 300, n_init = 10 , random_state = 0)
kmeans.fit(x)

"""# Now, lets visualize the clusters for each pair of features in our dataset"""

#@title adding a new column with predicted cluster value.


# adding a column y_kmeans to our dataset which has the predicted value of kmeans
iris_df['y_kmeans'] = kmeans.predict(x)

# as the y_kmeans will have the values 0,1,2 (as number of clusters are 3), replacing these values with group-1,group-2,group-3 respectively 
iris_df['y_kmeans'].replace({0:'group-1',1:'group-2',2:'group-3'},inplace = True)

#@title visualizing the clusters for each pair of features

sns.pairplot(iris_df,hue='y_kmeans')

"""**From above observations and pair plots, it is quite good to assume the optimum number of clusters to be equal to 3**

# THANK YOU
"""